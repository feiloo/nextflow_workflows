manifest {
  homePage = 'https://github.com/feiloo/ngs_pipeline'
  description = 'nextflow pipeline for somatic ngs analysis on tumor-normal wgs'
  mainScript = 'main.nf'
  version = '0.2.0'
}


nextflow.enable.strict = true
nextflow.enable.moduleBinaries = true
nextflow.enable.dsl = 2
workDir = "$NEXTFLOW_WORKDIR_CUSTOM"
offline = true
cleanup = false

// create unique workflow ids through timestamps
// the ids are used for structured outputs
import java.time.Instant
Instant inst = Instant.now()
workflow_id = inst.toString()

// by default, give every tool all processors, in case they use/benefit from them
// set different values when running non-locally, see profile below
//default_cpus = Runtime.runtime.availableProcessors()
default_cpus = 8
// the runtime.maxMemory is 1/4 of total ram, bc thats what the java runtime uses
//executor.memory = 0.8 * 4 * Runtime.runtime.maxMemory()
default_memory = "56.GB"

// profiles for are mutually exclusive for simplicity
// when needing a combination or different profile, just add it as its own profile here
// some example profiles:

// local, single-node, dependencies via conda
//scratch_dir = "$NEXTFLOW_SCRATCH_DIR"
// TODO ensure empty string evals to false
def scratch_var = System.getenv('NEXTFLOW_SCRATCH_DIR')
scratch_dir = (scratch_var != null && !scratch_var.trim().isEmpty()) ? scratch_var : false

// set default resources for small unimportant processes. (overwritten by process directives)
process {
  // deactivate publishing by default, so we dont spam outputs and waste storage
  publishDir = ["enabled":false]
  cpus = "${default_cpus}"
  memory = "${default_memory}"
  // use NXF_EXECUTOR to specify default executor
  //executor = 'local'
  time = 167.h
  errorStrategy = 'finish'
  scratch = '${scratch_dir}'

  // overwrite lower prio defaults with these higher prio defaults
  // .* regex for all processes
  withName: ".*" {
    // todo find tighter and process specific time bounds
    // since it improves slurm scheduling decisions
    time = 167.h
    // options for when slurm is used, use withname to overpower cio-variantinterpretation config
    clusterOptions = ['--oversubscribe','--nice=10']
    resourceLimits = [ cpus: 56, memory: 960.GB, time: 240.h ]
  }
}
// conda is our default
conda {
  enabled = true
  // set to .cache to keep the conda cache when wiping the nextflow_workdir
  conda.cacheDir = "$HOME/.cache/nextflow_conda_cache"
}

includeConfig "$NEXTFLOW_MODULES/variantinterpretation/nextflow.config"

params {
  // general input-output params
  output_dir = "$NEXTFLOW_OUTPUTDIR_CUSTOM/${workflow_id}"
  outdir = "$NEXTFLOW_OUTPUTDIR_CUSTOM/${workflow_id}"
  workdir = "$NEXTFLOW_WORKDIR_CUSTOM"
  workflow_run_id = "${workflow_id}"


  // full paths, to where inputs and outputs should be moved to on the network attached storage
  nas_import_dir = "$NAS_IMPORT_DIR"
  nas_export_dir = "$NAS_EXPORT_DIR"

  // references and other data
  reference_data = "$NGS_REFERENCE_DIR/arriba_reference/GRCh38+GENCODE38/"

  // gatk reference genome
  refgenome = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"

  //gatk intervals
  intervals = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/intervals/wgs_calling_regions_noseconds.hg38.bed"

  //gatk panel of normals
  panel_of_normals = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz"

  // germline resource (gnomad)
  germline_resource = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz"

  // dbsnp reference for gatk recalibration
  known_sites = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz"

  // clc tool specific params
  clc_workflow_name = '"name_of_clc_workflow"'

  // full filepaths to the import export directories on the clc server master node
  clc_import_dir = ''
  clc_export_dir = ''
  clc_destdir = ''

  // pancancer scripts variables
  vep_refgenome = "$NGS_REFERENCE_DIR/vep_fasta/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz"

  // cio variantinterpretation specific params
  fasta = "$NGS_REFERENCE_DIR/ncbi_grch38_wholegenomefasta_genome.fa"
  // vep_cache = "$NGS_REFERENCE_DIR/vep_cache/113/indexed/homo_sapiens_vep_113_GRCh38.tar.gz"
  vep_cache = "$NGS_REFERENCE_DIR/vep_cache/113/indexed/homo_sapiens_merged_vep_113_GRCh38.tar.gz"

  // additional cio+ukb wgs pilot specific annotation settings
  mskcc = ""
  refseq_list = "/home/pipeline_user/cio_ukb_wxs/variantinterpretation/nextflow_calldir/03022025_final_wxsRefSeq.txt"
  variantDBi = "/home/pipeline_user/Variantenliste22_12_15.xlsx"

  // bwa params
  // run slower but save memory by using bwa (esp on indexing), not bwa2 (bwamem2)
  bwa_tool = 'bwa2'

}

report {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/report.html"
}

timeline {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/timeline.html"
}

trace {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/trace.txt"
}

dag {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/dag.dot"
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit

// needed only for cio and nfcore bloat
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
