manifest {
  homePage = 'https://github.com/feiloo/ngs_pipeline'
  description = 'nextflow pipeline for somatic ngs analysis on tumor-normal wgs'
  mainScript = 'main.nf'
  version = '0.2.0'
}


nextflow.enable.strict = true
nextflow.enable.moduleBinaries = true
nextflow.enable.dsl = 2
workDir = "$NEXTFLOW_WORKDIR_CUSTOM"
offline = true
cleanup = false

// create unique workflow ids through timestamps
// the ids are used for structured outputs
import java.time.Instant
Instant inst = Instant.now()
workflow_id = inst.toString()

// by default, give every tool all processors, in case they use/benefit from them
// set different values when running non-locally, see profile below
//default_cpus = Runtime.runtime.availableProcessors()
default_cpus = 8
// the runtime.maxMemory is 1/4 of total ram, bc thats what the java runtime uses
//executor.memory = 0.8 * 4 * Runtime.runtime.maxMemory()
default_memory = "56.GB"

// profiles for are mutually exclusive for simplicity
// when needing a combination or different profile, just add it as its own profile here
// some example profiles:

// local, single-node, dependencies via conda
//scratch_dir = "$NEXTFLOW_SCRATCH_DIR"
// TODO ensure empty string evals to false
def scratch_var = System.getenv('NEXTFLOW_SCRATCH_DIR')
def scratch_dir = (scratch_var != null && !scratch_var.trim().isEmpty()) ? scratch_var : false

// ugly hack, but ensures that nextflow doesnt munge a invalid scratch directory to /tmp
// as /tmp can be costly because its fills and crashes slurm through a full spool dir (iff on same node)
if (scratch_dir) {
  def dir1 = new File(scratch_dir)
  if (!dir1.exists() || !dir1.isDirectory()) {
    throw new IOException("Scratch Directory invalid: $scratch_dir")
  }
}

// set default resources for small unimportant processes. (overwritten by process directives)
process {
  // deactivate publishing by default, so we dont spam outputs and waste storage
  publishDir = ["enabled":false]
  cpus = "${default_cpus}"
  memory = "${default_memory}"
  // use NXF_EXECUTOR to specify default executor
  executor = 'slurm'
  time = 167.h
  errorStrategy = {task.attempt < 3 ? 'retry' : 'ignore'}
  maxRetries = 5
  scratch = "${scratch_dir}"

  // overwrite lower prio defaults with these higher prio defaults
  // .* regex for all processes
  withName: ".*" {
    // todo find tighter and process specific time bounds
    // since it improves slurm scheduling decisions
    time = 167.h
    // options for when slurm is used, use withname to overpower cio-variantinterpretation config
    clusterOptions = ['--oversubscribe','--nice=10']
    resourceLimits = [ cpus: 56, memory: 960.GB, time: 240.h ]
  }

  withName: "gatk_learn_readorientationmodel" {
      memory = {(120 + 40 * (task.attempt-1)).GB}
  }
  withName: "UKB_FILTER" {
      memory = {(40 + 40 * (task.attempt-1)).GB}
  }
  withName: "UKB_REPORT" {
      memory = {(40 + 40 * (task.attempt-1)).GB}
  }

  withName: "publish" {
    cache = false
    time = {task.attempt < 2 ? "2 hours 56 minutes " : 167.h}
  }

  // the following entries were semi-generated from statistics
  withName: "sequence_alignment:fastp" {
          time = {task.attempt < 2 ? "1 days 19 hours 28 minutes " : 167.h}
          }
  withName: "bwamem2_align" {
          time = {task.attempt < 2 ? "1 days 18 hours 17 minutes " : 167.h}
          queue = "cpu_hpc"
          }
  withName: "gatk_baserecalibrator" {
          time = {task.attempt < 2 ? "1 days 12 hours 56 minutes " : 167.h}
          }
  withName: "gatk_apply_bqsr" {
          time = {task.attempt < 2 ? "1 days 6 hours 12 minutes " : 167.h}
          }
  withName: "gatk_markduplicates" {
          time = {task.attempt < 2 ? "1 days " : 167.h}
          queue = "cpu_hpc"
          }
  withName: "collect_hs_metrics" {
          time = {task.attempt < 2 ? "19 hours 30 minutes " : 167.h}
          }
  withName: "gatk_getpileupsummaries" {
          time = {task.attempt < 2 ? "18 hours 49 minutes " : 167.h}
          }
  withName: "gatk_mutect_shard" {
          time = {task.attempt < 2 ? "10 hours 48 minutes " : 167.h}
          queue = "cpu_hpc"
          }
  withName: "gatk_set_tags" {
          time = {task.attempt < 2 ? "8 hours 33 minutes " : 167.h}
          }
  withName: "eval_msi" {
          time = {task.attempt < 2 ? "7 hours 59 minutes " : 167.h}
          }
  withName: "DATAVZRD" {
          time = {task.attempt < 2 ? "7 hours 35 minutes " : 167.h}
          }
  withName: "bam_stats" {
          time = {task.attempt < 2 ? "4 hours 33 minutes " : 167.h}
          }
  withName: "bam_coverage" {
          time = {task.attempt < 2 ? "4 hours 3 minutes " : 167.h}
          }
  withName: "sort_bam" {
          time = {task.attempt < 2 ? "2 hours 45 minutes " : 167.h}
          }
  withName: "gatk_learn_readorientationmodel" {
          time = {task.attempt < 2 ? "2 hours 18 minutes " : 167.h}
          }
  withName: "bam_depth" {
          time = {task.attempt < 2 ? "1 hours 54 minutes " : 167.h}
          }
  withName: "gatk_filter_calls" {
          time = {task.attempt < 2 ? "1 hours 10 minutes " : 167.h}
          }
}
// conda is our default
conda {
  enabled = true
  // set to .cache to keep the conda cache when wiping the nextflow_workdir
  conda.cacheDir = "$HOME/.cache/nextflow_conda_cache"
}

includeConfig "$NEXTFLOW_MODULES/variantinterpretation/nextflow.config"

params {
  // general input-output params
  output_dir = "$NEXTFLOW_OUTPUTDIR_CUSTOM/${workflow_id}"
  outdir = "$NEXTFLOW_OUTPUTDIR_CUSTOM/${workflow_id}"
  workdir = "$NEXTFLOW_WORKDIR_CUSTOM"
  workflow_run_id = "${workflow_id}"


  // full paths, to where inputs and outputs should be moved to on the network attached storage
  nas_import_dir = "$NAS_IMPORT_DIR"
  nas_export_dir = "$NAS_EXPORT_DIR"

  // references and other data
  reference_data = "$NGS_REFERENCE_DIR/arriba_reference/GRCh38+GENCODE38/"

  // gatk reference genome
  refgenome = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"

  //gatk intervals
  intervals = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/intervals/wgs_calling_regions_noseconds.hg38.bed"

  //gatk panel of normals
  panel_of_normals = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz"

  // germline resource (gnomad)
  germline_resource = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz"

  // dbsnp reference for gatk recalibration
  known_sites = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz"

  // clc tool specific params
  clc_workflow_name = '"name_of_clc_workflow"'

  // full filepaths to the import export directories on the clc server master node
  clc_import_dir = ''
  clc_export_dir = ''
  clc_destdir = ''

  // pancancer scripts variables
  vep_refgenome = "$NGS_REFERENCE_DIR/vep_fasta/Homo_sapiens.GRCh38.dna.primary_assembly.fa.gz"

  // cio variantinterpretation specific params
  fasta = "$NGS_REFERENCE_DIR/ncbi_grch38_wholegenomefasta_genome.fa"
  // vep_cache = "$NGS_REFERENCE_DIR/vep_cache/113/indexed/homo_sapiens_vep_113_GRCh38.tar.gz"
  vep_cache = "$NGS_REFERENCE_DIR/vep_cache/113/indexed/homo_sapiens_merged_vep_113_GRCh38.tar.gz"

  // additional cio+ukb wgs pilot specific annotation settings
  mskcc = ""
  refseq_list = "/home/pipeline_user/cio_ukb_wxs/variantinterpretation/nextflow_calldir/03022025_final_wxsRefSeq.txt"
  variantDBi = "/home/pipeline_user/Variantenliste22_12_15.xlsx"

  // bwa params
  // run slower but save memory by using bwa (esp on indexing), not bwa2 (bwamem2)
  bwa_tool = 'bwa2'

}

report {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/report.html"
}

timeline {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/timeline.html"
}

trace {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/trace.txt"
}

dag {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/dag.dot"
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit

// needed only for cio and nfcore bloat
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
