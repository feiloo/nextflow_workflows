manifest {
  homePage = 'https://github.com/feiloo/ngs_pipeline'
  description = 'nextflow pipeline for somatic ngs analysis on tumor-normal wgs'
  mainScript = 'main.nf'
  version = '0.0.1'
}


nextflow.enable.strict = true
nextflow.enable.moduleBinaries = true
nextflow.enable.dsl = 2
workDir = "$NEXTFLOW_WORKDIR_CUSTOM"
offline = true
cleanup = false


// create unique workflow ids through timestamps
// the ids are used for structured outputs
import java.time.Instant
Instant inst = Instant.now()
workflow_id = inst.toString()

// by default, give every tool all processors, in case they use/benefit from them
// set different values when running non-locally, see profile below
//default_cpus = Runtime.runtime.availableProcessors()
default_cpus = 8
// the runtime.maxMemory is 1/4 of total ram, bc thats what the java runtime uses
//executor.memory = 0.8 * 4 * Runtime.runtime.maxMemory()
// in GB
default_memory = 56

// profiles for are mutually exclusive for simplicity
// when needing a combination or different profile, just add it as its own profile here
// some example profiles:
profiles {
  standard {
    // local, single-node, dependencies via conda

    // deactivate publishing by default, so we dont spam outputs and waste storage
    // set default resources for small unimportant processes. (overwritten by process directives)
    process {
      publishDir = ["enabled":false]
      withName: ".*" {
          time = 167.h
      }

      cpus = "${default_cpus}"
      memory = { 56.GB * task.attempt }
      cache = 'lenient'
      errorStrategy = 'retry'
      maxRetries = 4

      time = 167.h
      executor = 'slurm'
      //scratch = "$NEXTFLOW_SCRATCH_DIR"

    }
    conda {
      enabled = true
      // set to .cache to keep the conda cache when wiping the nextflow_workdir
      //conda.cacheDir = "/.cache/nextflow_conda_cache"
    }
  }

  fully_native {
    // local, single-node, dependencies must be installed locally and be on PATH env-var

    // deactivate publishing by default, so we dont spam outputs and waste storage
    // set default resources for small unimportant processes. (overwritten by process directives)
    process {
      publishDir = ["enabled":false]
      withName: '.*' {
              cpus = "${default_cpus}"
              memory = "${default_memory}"
      }
    }
  }


  local_nas_workdir {
    // local, single-node, conda, use copy for staging for loading data from nas-filesystems
    // deactivate publishing by default, so we dont spam outputs and waste storage
    // set default resources for small unimportant processes. (overwritten by process directives)
    process {
      publishDir = ["enabled":false]
      withName: '.*' {
              cpus = "${default_cpus}"
              memory = "${default_memory}"
      }
      // nas filesystems dont support symlinks, therefore use scratch and stage files through copying
      stageInMode = 'copy'
      stageOutMode = 'copy'
      // use local scratch for fast io and avoid symlink issues
      scratch = '/scratch'
    }

    conda {
      enabled = true
      // set to .cache to keep the conda cache when wiping the nextflow_workdir
      conda.cacheDir = "$HOME/.cache/nextflow_conda_cache"
    }
  }

  hpc_localstorage {
    // run multinode with slurm and conda

    // set default resources for small unimportant processes. (overwritten by process directives)
    process {
      publishDir = ["enabled":false]
      withName: '.*' {
              cpus = 8
              memory = "${default_memory}"
      }
      // nas filesystems dont support symlinks, therefore use scratch and stage files through copying
      scratch = '/scratch'
      executor = 'slurm'
    }
    conda {
      enabled = true
      // set to .cache to keep the conda cache when wiping the nextflow_workdir
      conda.cacheDir = "$HOME/.cache/nextflow_conda_cache"
    }
  }

  hpc_nas {
    // run multinode with slurm and conda and load data from nas

    // set default resources for small unimportant processes. (overwritten by process directives)
    process {
      publishDir = ["enabled":false]
      withName: '.*' {
              cpus = 8
              memory = "${default_memory}"
      }
      // nas filesystems dont support symlinks, therefore use scratch and stage files through copying
      stageInMode = 'copy'
      stageOutMode = 'copy'
      scratch = '/scratch'
      executor = 'slurm'
    }
    conda {
      enabled = true
      // set to .cache to keep the conda cache when wiping the nextflow_workdir
      conda.cacheDir = "$HOME/.cache/nextflow_conda_cache"
    }
  }
}

includeConfig "$NEXTFLOW_MODULES/variantinterpretation/nextflow.config"

params {
  // general input-output params
  output_dir = "$NEXTFLOW_OUTPUTDIR_CUSTOM/${workflow_id}"
  outdir = "$NEXTFLOW_OUTPUTDIR_CUSTOM/${workflow_id}"
  workdir = "$NEXTFLOW_WORKDIR_CUSTOM"

  // full paths, to where inputs and outputs should be moved to on the network attached storage
  nas_import_dir = "$NAS_IMPORT_DIR"
  nas_export_dir = "$NAS_EXPORT_DIR"

  // references and other data
  reference_data = "$NGS_REFERENCE_DIR/arriba_reference/GRCh38+GENCODE38/"

  // gatk reference genome
  refgenome = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta"

  //gatk intervals
  intervals = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/intervals/wgs_calling_regions_noseconds.hg38.bed"

  //gatk panel of normals
  panel_of_normals = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/1000g_pon.hg38.vcf.gz"

  // germline resource (gnomad)
  germline_resource = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/af-only-gnomad.hg38.vcf.gz"

  // dbsnp reference for gatk recalibration
  known_sites = "$NGS_REFERENCE_DIR/sarek_reference/gatk_grch38/Homo_sapiens/GATK/GRCh38/Annotation/GATKBundle/dbsnp_146.hg38.vcf.gz"

  // clc tool specific params
  clc_workflow_name = '"name_of_clc_workflow"'

  // full filepaths to the import export directories on the clc server master node
  clc_import_dir = ''
  clc_export_dir = ''
  clc_destdir = ''

  // cio variantinterpretation specific params
  fasta = "$NGS_REFERENCE_DIR/ncbi_grch38_wholegenomefasta_genome.fa"
  vep_cache = "$NGS_REFERENCE_DIR/vep_cache/113/indexed/homo_sapiens_vep_113_GRCh38.tar.gz"

  // additional cio+ukb wgs pilot specific annotation settings
  mskcc = ""

  // private internal list of variants
  // sha256sum: a6eb382d746229b63563421372a27450930c8be6522cf563d8b01868354a0247
  // md5sum: 5d0680a05055db08b1a9887aae339f42
  refseq_list = "/home/pipeline_user/12032025_use_in_wgs_pilot_refseq.txt"

  // sha256sum: 5a964ba2a90dfbcb6724b535d17086bc9f5827043ffdaae6bfeeadd809a876dc
  // md5sum: 25e0af6e765cb42e2a3d19d850770466
  variantDBi = "/home/pipeline_user/Variantenliste22_12_15.xlsx"

  UKB_report = null
  UKB_filter = 1

  // bwa params
  // run slower but save memory by using bwa (esp on indexing), not bwa2 (bwamem2)
  bwa_tool = 'bwa2'

}

report {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/report.html"
}

timeline {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/timeline.html"
}

trace {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/trace.txt"
}

dag {
  enabled = true
  overwrite = true
  file = "${params.output_dir}/dag.dot"
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
